{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twUL4J3zk7Zk"
   },
   "source": [
    "# Tiny LLM Story Generator — Training Notebook\n",
    "\n",
    "**Purpose:** This notebook trains a compact GPT-2 style language model to generate short children’s stories using the **TinyStories** dataset. It covers data loading, tokenization, model configuration, custom training, checkpointing, and sampling from saved checkpoints.\n",
    "\n",
    "## What this notebook does\n",
    "1. **Setup (Colab + Dependencies):** Mount Google Drive for persistent storage and import core libraries (`transformers`, `datasets`, `torch`, etc.).  \n",
    "2. **Data:** Load `roneneldan/TinyStories` via Hugging Face Datasets and perform lightweight preprocessing/tokenization suitable for small-context language modeling.  \n",
    "3. **Model:** Initialize a small GPT-2 configuration (tokenizer + `GPT2LMHeadModel`) tailored for fast prototyping on limited resources.  \n",
    "4. **Training Loop:** Train with `AdamW`, gradient clipping, and mini-batches using `DataLoader`/`IterableDataset`; track loss and save periodic checkpoints.  \n",
    "5. **Logging & Plots:** Record training history (e.g., loss) and visualize progression to validate convergence.  \n",
    "6. **Checkpointing:** Persist tokenizer/model to Drive for later reuse and reproducibility.  \n",
    "7. **Inference:** Load a chosen checkpoint and generate stories to qualitatively evaluate results.\n",
    "\n",
    "## Why TinyStories?\n",
    "TinyStories is a curated corpus of short, simple narratives designed for training and evaluating small language models. It enables rapid experiments while demonstrating end-to-end LM training and text generation.\n",
    "\n",
    "## Requirements\n",
    "- Python 3.x, PyTorch, Transformers, Datasets, TQDM, Matplotlib  \n",
    "- Sufficient GPU (e.g., Colab T4/A100) recommended\n",
    "\n",
    "## Reproducibility & Tips\n",
    "- Fix random seeds for consistent runs.  \n",
    "- Start with a small context length and batch size; scale up gradually.  \n",
    "- Monitor loss curves; stop early if overfitting.  \n",
    "- Keep checkpoints versioned (e.g., `tinygpt2_epochN`).\n",
    "\n",
    "> **Reference Dataset:** `roneneldan/TinyStories` (Hugging Face Datasets).  \n",
    "> **Author:** Ashish (Data Science Mentor) — YYYY-MM-DD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUFbywcYlKB_"
   },
   "source": [
    "### 1. Google Drive Mount\n",
    "\n",
    "Mounts Google Drive in Colab to access and save files directly from your Drive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UDGsTaALLb7d",
    "outputId": "3baa5f49-2f60-42b1-be72-0e4f3e2c036a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ie1y9C0llSOg"
   },
   "source": [
    "### 2. Library Installation and Data Loading\n",
    "\n",
    "- Installs the **`datasets`** library.  \n",
    "- Suppresses warning messages for cleaner output.  \n",
    "- Imports essential libraries for data handling, tokenization, visualization, and model building.  \n",
    "- Loads the **TinyStories** dataset in streaming mode for training.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yS_ATZUmQYn5"
   },
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import re\n",
    "import torch\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrAASV-ZlcPq"
   },
   "source": [
    "### 3. TinyStoriesStreamDataset Class\n",
    "\n",
    "- Creates a **streaming PyTorch dataset** for TinyStories text.  \n",
    "- Steps performed for each story:\n",
    "  1. **Skip short samples:** Stories shorter than `min_length` are ignored.  \n",
    "  2. **Clean text:**  \n",
    "     - Removes extra spaces and unwanted characters.  \n",
    "     - Replaces fancy quotes with standard quotes.  \n",
    "  3. **Tokenize:** Converts text into token IDs using a GPT-2 tokenizer.  \n",
    "  4. **Prepare training inputs:**  \n",
    "     - `input_ids`: All tokens except the last one.  \n",
    "     - `labels`: All tokens except the first one (for next-token prediction).  \n",
    "     - `attention_mask`: Marks which tokens are real vs. padding.  \n",
    "\n",
    "\n",
    "\n",
    "#### Example\n",
    "    **Input text:**  \n",
    "    `\"  “The dog runs!” said Tom.  \"`  \n",
    "\n",
    "    **After cleaning:**  \n",
    "    `\"The dog runs!\" said Tom.`  \n",
    "\n",
    "    **Tokenization output (IDs):**  \n",
    "    `[50256, 464, 3290, 1101, 0, 616, 640, 13]`  \n",
    "\n",
    "    **Prepared for training:**  \n",
    "    | input_ids                | labels                    |\n",
    "    |--------------------------|---------------------------|\n",
    "    | [50256, 464, 3290, 1101] | [464, 3290, 1101, 0]      |\n",
    "\n",
    "    This way, the model learns to predict the **next token** at each position.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uEgDK6OPQdcH"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "class TinyStoriesStreamDataset(IterableDataset):\n",
    "    def __init__(self, dataset_stream, tokenizer, block_size=512, min_length=30):\n",
    "        self.dataset = dataset_stream\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        self.min_length = min_length\n",
    "\n",
    "    def __iter__(self):\n",
    "        for sample in self.dataset:\n",
    "            text = sample[\"text\"].strip()\n",
    "            if len(text) < self.min_length:\n",
    "                continue\n",
    "\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            text = re.sub(r'[“”]', '\"', text)\n",
    "            text = re.sub(r\"[‘’]\", \"'\", text)\n",
    "            text = re.sub(r'[^a-zA-Z0-9.,!?\\'\"\\s]', '', text)\n",
    "\n",
    "            tokenized = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                add_special_tokens=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.block_size,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            input_ids = tokenized[\"input_ids\"][0]\n",
    "            attention_mask = tokenized[\"attention_mask\"][0]\n",
    "\n",
    "            yield {\n",
    "                \"input_ids\": input_ids[:-1],\n",
    "                \"labels\": input_ids[1:],\n",
    "                \"attention_mask\": attention_mask[:-1]\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2Y10fFcltP9"
   },
   "source": [
    "### 4. Load Tokenizer, DataLoader, Model, and Optimizer Setup\n",
    "\n",
    "1. **Training size & batching**\n",
    "   - Define total samples and `batch_size`; compute `max_batches_per_epoch` for progress tracking.\n",
    "\n",
    "2. **Tokenizer**\n",
    "   - Load GPT-2 tokenizer and set the **pad token** to EOS for consistent padding.\n",
    "\n",
    "3. **Streaming dataset → DataLoader**\n",
    "   - Wrap `TinyStoriesStreamDataset` with a `DataLoader` to yield mini-batches for training.\n",
    "\n",
    "4. **Model configuration**\n",
    "   - Build a **small GPT-2**:\n",
    "     - `vocab_size = len(tokenizer)`\n",
    "     - Context length: `n_positions = n_ctx = 512`\n",
    "     - Model width: `n_embd = 256`\n",
    "     - Depth/heads: `n_layer = 4`, `n_head = 4`\n",
    "     - Use tokenizer’s `pad_token_id`\n",
    "\n",
    "5. **Device placement**\n",
    "   - Move model to **GPU** if available; enable **DataParallel** when multiple GPUs exist.\n",
    "\n",
    "6. **Optimizer**\n",
    "   - Initialize **AdamW** with learning rate `5e-5` for stable transformer training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "5dac7ea789da43388d01bc4f5702fdcc",
      "62d05befe34746609820491ab5463455",
      "0641236e3f394a84be3787b8c01580cf",
      "8d10a6b0aaec46488ea739cf9e6181d6",
      "2418ee4963f1483b956571846f9186e4",
      "5aafa05f42184f4db7946e26771a07e6",
      "ae018c9355c444b9a43b864c3325afab",
      "330cb5e59a524fed8f73062640e69dcc",
      "5675e97ae6514df7823636e9d04cffae",
      "0f77ee3ba0404bb99c49203d19f00cca",
      "5c190713e6c84791bbec08ba25077fcd",
      "02f5de5a8c97463e9518627c6249eedd",
      "ad301dea0a0f4ffb82b428cb39b8eee3",
      "a42c6ce5c6794919ad2eff7cad85ee53",
      "57f468bf81d64db4a0337d50b3fe4e37",
      "b9ec5eee470b4c9e9067c0bf74a62be9",
      "40319eded5694b66baeba8574a29898d",
      "69f18bfadb8246eb87a1b119f8cff7e6",
      "bab8564b815c43709ff5867d2e368331",
      "edd13d55ed7241a8925fae0d23d9ae6f",
      "5f41ff516c9b4a968294edf3f4739f05",
      "3de3e87c1a2544249b86b76f1e216216",
      "23315faf5f424b759bd1912b7cc8b6ba",
      "1c9f144711884ad593524940198fe40f",
      "02a95835922441dab26395781a6a1a55",
      "e040d867351b47e3aed1261cd4b23ac8",
      "96a0e13afd134c27b617b9bf272fe014",
      "2de5570c64bb4fbeb74eaeee16223909",
      "7723ea961d7944cb8a046bf2ee10df9f",
      "f703ddeb513340f4b7d2272537c6cd63",
      "a83897974f0446ffa02ad3e75b84b72d",
      "8a05ad07496143d7adb700424381fdb4",
      "995105badb244f0a895f70cc1cbd655b",
      "3c354ead81624f30b69162eb2048d077",
      "0f885fb239ff49a5866f5549cc7b7840",
      "6c74410180594270b59c25a72be3e5ad",
      "b9476084fbac4ba79306b2046deffbde",
      "94df48b44904403baad814e5acbc0365",
      "54cb1923d222473c92e63d696d3e4852",
      "b7e215caf8204b13891f701a2aeb2d4e",
      "57d2b69c27c34e769cdcefb9782a95a8",
      "6fe30d96ae5d423dba61b0002e6464ab",
      "8ea3c8c369e44498bcdfed9c8c788ff2",
      "cbed3e76318e4da5b539c5a945d2eee4",
      "8ffdd963b60e4dcfaacd36b0c8fd163e",
      "a44066e566fc42d2bcd0fa18fb07df98",
      "d720cba388514104ac6579c90c8d590e",
      "99a9ea09086a4dc680f001e8bca8eb0a",
      "4d2cdb61e54c448e92b7ba1434f40265",
      "4d4e10c4726d45788a1326934081136b",
      "f5cc3d6df0f946cda908a381f0204997",
      "f6acb0018ffe47c5af53df3ecd61e337",
      "6c15b7d94c1c479c8711b3c8c01069fd",
      "e00e42575de343c19cb203c22d7582cc",
      "637fe711f1f845789591d61acd2645ee"
     ]
    },
    "id": "Q0vYQUpvQfk6",
    "outputId": "26d28a71-05ee-4463-9194-ad38f141b409"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dac7ea789da43388d01bc4f5702fdcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f5de5a8c97463e9518627c6249eedd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23315faf5f424b759bd1912b7cc8b6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c354ead81624f30b69162eb2048d077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ffdd963b60e4dcfaacd36b0c8fd163e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "total_samples = 2119719\n",
    "batch_size = 52\n",
    "max_batches_per_epoch = total_samples // batch_size\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "stream_dataset = TinyStoriesStreamDataset(dataset, tokenizer)\n",
    "train_loader = DataLoader(stream_dataset, batch_size=batch_size)\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_positions=512,\n",
    "    n_ctx=512,\n",
    "    n_embd=256,\n",
    "    n_layer=4,\n",
    "    n_head=4,\n",
    "    pad_token_id=tokenizer.pad_token_id)\n",
    "\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Li16Hcuul7Tr"
   },
   "source": [
    "### 5. Training Loop, Checkpointing, and Sampling\n",
    "\n",
    "1. **Setup**\n",
    "   - Define a checkpoint folder on Google Drive.\n",
    "   - Set number of epochs and initialize a loss history list.\n",
    "   - Switch model to training mode.\n",
    "\n",
    "2. **Epoch training**\n",
    "   - For each epoch:\n",
    "     - Iterate over mini-batches up to `max_batches_per_epoch`.\n",
    "     - Move tensors to the selected device (CPU/GPU).\n",
    "     - Compute loss with labels for next-token prediction.\n",
    "     - Zero gradients → backpropagate → clip gradients (max norm = 1.0) → optimizer step.\n",
    "     - Accumulate batch losses.\n",
    "\n",
    "3. **Track progress**\n",
    "   - Compute and log **average loss** per epoch.\n",
    "   - Append the epoch’s average loss to `history`.\n",
    "\n",
    "4. **Checkpointing**\n",
    "   - Create an epoch-specific folder (e.g., `tinygpt2_epochN`).\n",
    "   - Save both the **model** and **tokenizer** to Drive after every epoch.\n",
    "\n",
    "5. **Qualitative check (sampling)**\n",
    "   - Temporarily switch to eval mode.\n",
    "   - Generate a short continuation from the prompt *“Once upon a time”*.\n",
    "   - Print the generated text to inspect model quality, then return to train mode.\n",
    "\n",
    "6. **Persist training history**\n",
    "   - Save the list of epoch losses to `training_history.json` on Drive for later plotting or review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 955,
     "referenced_widgets": [
      "a1e1ac139ece47e9ad6f8db825c6e008",
      "b2565d495915459e9256f2a6e0f09adf",
      "5e8a93c24c5c46c193dff5f51f575402",
      "67060ce45367493b95a47466c98a6a12",
      "728a83790b2a4155a2541b618cead5d4",
      "4c92496a56a04ee9aafe98af48c38c86",
      "58672c68571e400d9236870e81d89c61",
      "41b86d155e854dc2b2fed43e7ee14fff",
      "91c44a4316e940a08dbd6103b92c1071",
      "cf9da7a840d644fe84e71338dbc0a469",
      "85020e596e5c42fd8818eefbf7649f72",
      "30b8e7a44a3d4823b377e525c5a53b57",
      "ab286d7c03ed410f9f8173fd5abea200",
      "47e01e1095a64623a6769ae9b687df7f",
      "19ec48b9678f432888ba58d4e12628d1",
      "f6489cee07104297ae081de4124bad02",
      "32f8fae18a0247d48f5440de18976c3e",
      "c538c1c932764a718aa97179d828ea02",
      "b81898185e3144cca131f6e78b5b72a3",
      "db67909bae2d498aba6ccf25b91f583e",
      "fe3dade7820047d49d1f1a7d0efa487a",
      "50acf8741f604271823697a6aa419a58",
      "a3af8d43bbde495f8aabfe832375bc53",
      "19d26a96453a48198c5df002e238d8dd",
      "2f6971a62d234e5c913979f61b2745dc",
      "750dadf0e3e5421dba6b6f8a5045dc24",
      "f560deee06f847699f0a34c88d3ac44a",
      "ac0de30049964ec9a8d77655687892e1",
      "f8f25ff6d3b8412d861b1edc10c8f7a5",
      "c04aa67afb344806b69568d77ab59200",
      "68840e6450394262a5de8547b5e2eed5",
      "f5646aab0d2e4d30a819773f318cae5f",
      "f0e171f64db346ca8963a85773fe10cb",
      "156e3733795c4931b0fb336c95ac036a",
      "5d817ebf99944346a3fdeaea241e01b5",
      "4a24f32a50cd4f94afba88678c0b1ae3",
      "22f2802c7035430ab262653ad200a859",
      "0d1958b6b7f74c108937f363b7954999",
      "0e182d4dadb04ffa95ccd00502271401",
      "0ca25810800b42488331dcb640e2d664",
      "9d78a841519248d6bba7ea13abf2c15b",
      "f60b1a6a461f42a693dd8caaf92240ea",
      "fa6fc81025654d0b9a284796130dd331",
      "0b78d00875af48a9a03ac23796e71aab",
      "6c462f2413a649bd9bba027830843da9",
      "3d26853b90094a5b8ded7c870134938a",
      "41f4be7bbf8b41b699deb18b0985c4a0",
      "a30e89268bc240c5b5cb1fdcfa0a789b",
      "436ff59b82dd44c2a074fd07121d49ab",
      "a3c86422b91e4fc1873be501df1a95a0",
      "d63b6cbedb3d4ae191d57ae9f5f2f854",
      "3d3b58c6a7514143aa1fefe0a7a1f26e",
      "4b89bb39134f432ca23426005d83f427",
      "f0a9d53dd87147c7bef8bf11974f8790",
      "54c216dbf2cc49ac909a22efd768d409",
      "08a46323874742c0beaf51c2ab23e52b",
      "92ba2a00190b41379611e5f605e80336",
      "960b222db79d4dd18d9cd096621dde7d",
      "b7cf1b54d0e647129df2b6dc14e7db44",
      "7b2f3e4273ed445aaa7bc6de98f63e66",
      "1704745a32494cf8a8c7dea3928d9249",
      "1e3ab91f9fac4a958fe1180bd5caaf56",
      "b03b4e321d394e4f8d62b06de32277bb",
      "6413bd867165446cbb4c776c98dc761b",
      "f616ecadcf93416b9b283541a164387b",
      "a31781302bda41d7874b3fb05cc5d3b3",
      "a900eef992c14e31ad33abb2bad225ee",
      "3c424c6ad1c44294919997f772a9e3d3",
      "a67bd7a470e741f9a41f41cc7592aedd",
      "4730742d0a2149b68e5b40a0a5bbd891",
      "4d0882ba7f634f06a16d7191f805b46b",
      "b7707462e22d463894cf7c7a2852a19a",
      "11bdaa9954ed4b27b93dc62f2bb0426f",
      "818c78e200d54fbface3291bd1609485",
      "ea1bcb3cdebc4cf881c7d0a13063316a",
      "76208c5c6f284364b72cfcf98e8d9571",
      "fd8d375e395247e1b1d4186967b82682"
     ]
    },
    "id": "dgZF4U2nIkx_",
    "outputId": "4bc88622-984b-47ac-fc13-38c9af30f493"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e1ac139ece47e9ad6f8db825c6e008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40763 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.4778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved at /content/drive/MyDrive/TinyLLM/model/tinygpt2_epoch1\n",
      "Sample Output:\n",
      "Once upon a time there a girl Lily She to her's. was years and to. day she to with mom dad They to a. had big of. wanted play the, Lily her, she her and dad her said \",'s to your\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b8e7a44a3d4823b377e525c5a53b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40763 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.2364\n",
      "Model checkpoint saved at /content/drive/MyDrive/TinyLLM/model/tinygpt2_epoch2\n",
      "Sample Output:\n",
      "Once upon a time there a girl Lily She to her's. was years and to. day she to outside play her. saw big and, sky blue She so! wanted go and in. she her said her,Mom, I to in sky\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3af8d43bbde495f8aabfe832375bc53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40763 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.1914\n",
      "Model checkpoint saved at /content/drive/MyDrive/TinyLLM/model/tinygpt2_epoch3\n",
      "Sample Output:\n",
      "Once upon a time there a girl Lily She to her and mom in kitchen Lily to. mom her was a patient and her gave a. was patient patient waited waited the to and., waited her waited a minutes waited Finally the was. waited and\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "156e3733795c4931b0fb336c95ac036a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40763 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.1686\n",
      "Model checkpoint saved at /content/drive/MyDrive/TinyLLM/model/tinygpt2_epoch4\n",
      "Sample Output:\n",
      "Once upon a time there a girl Lily She to her and brother They to in park Lily a day They a and saw big. wanted go the, they to park. asked mom they go but said. said because were and. was and and to\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c462f2413a649bd9bba027830843da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40763 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.1542\n",
      "Model checkpoint saved at /content/drive/MyDrive/TinyLLM/model/tinygpt2_epoch5\n",
      "Sample Output:\n",
      "Once upon a time there a girl Lucy She three old and mom who to for walk the. they to park Lucy a and dad a. was excited go the and around park They a of. saw big and swing Lucy to on swing She to and\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a46323874742c0beaf51c2ab23e52b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40763 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.1440\n",
      "Model checkpoint saved at /content/drive/MyDrive/TinyLLM/model/tinygpt2_epoch6\n",
      "Sample Output:\n",
      "Once upon a time there a girl Lily She to her and mom for walk the. they a and girl to park Lily a. saw big and dog The was and. wanted pet dog but mom no was. said \",,. dog not.\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a900eef992c14e31ad33abb2bad225ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40763 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# Define checkpoint directory\n",
    "checkpoint_dir = Path(\"/content/drive/MyDrive/TinyLLM/model/\")\n",
    "\n",
    "epochs = 10\n",
    "history = []\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for i, batch in enumerate(tqdm(train_loader, total=max_batches_per_epoch)):\n",
    "        if i >= max_batches_per_epoch:\n",
    "            break\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, labels=labels, attention_mask=attention_mask)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / max_batches_per_epoch\n",
    "    history.append(avg_loss)\n",
    "    print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save model after every epoch\n",
    "    epoch_checkpoint = checkpoint_dir / f\"tinygpt2_epoch{epoch+1}\"\n",
    "    epoch_checkpoint.mkdir(parents=True, exist_ok=True)\n",
    "    model.save_pretrained(epoch_checkpoint)\n",
    "    tokenizer.save_pretrained(epoch_checkpoint)\n",
    "    print(f\"Model checkpoint saved at {epoch_checkpoint}\")\n",
    "\n",
    "    # Generate sample output\n",
    "    model.eval()\n",
    "    sample_input = tokenizer.encode(\"Once upon a time\", return_tensors=\"pt\").to(device)\n",
    "    generated_ids = model.generate(\n",
    "        sample_input,\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(f\"Sample Output:\\n{generated_text}\")\n",
    "    model.train()\n",
    "\n",
    "history_path = Path(\"/content/drive/MyDrive/TinyLLM/training_history.json\")\n",
    "with open(history_path, \"w\") as f:\n",
    "    json.dump(history, f)\n",
    "print(f\"\\nTraining history saved to {history_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ungs7_EHmElM"
   },
   "source": [
    "### 6. Resume Training from Checkpoint\n",
    "\n",
    "1. **Load checkpoint**\n",
    "   - Restore the model and tokenizer from `tinygpt2_epoch6`.\n",
    "\n",
    "2. **Configure training**\n",
    "   - Recreate optimizer, device placement (GPU if available), and batching parameters.\n",
    "\n",
    "3. **Continue epochs**\n",
    "   - Train from epoch 7 onward (up to the target `epochs`), repeating the standard loop:\n",
    "     - Forward pass → loss\n",
    "     - Zero grads → backward pass\n",
    "     - Gradient clipping (max norm = 1.0)\n",
    "     - Optimizer step\n",
    "\n",
    "4. **Checkpoint each epoch**\n",
    "   - Save model and tokenizer to `tinygpt2_epoch{N}` after every epoch.\n",
    "\n",
    "5. **Quick qualitative check**\n",
    "   - Switch to eval, generate a short continuation from “Once upon a time”, print sample, then return to train mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 889,
     "referenced_widgets": [
      "8a472d2c5cb344d2a04da2d037f65682",
      "d242654c1aad459ea6517303f4c56f98",
      "482e0a5ba4204eff8e8c5ab95eff0650",
      "5bef543ba7c24a5fb94d2b1485c93df3",
      "9014967b98c24aecaac7cdedb28f3e79",
      "93c4e00c665b497a8244f2e160ad427d",
      "2fb3201d51e14e0e9712799ed7d7070a",
      "235ce9688bfc403ebf1626548a8a4f07",
      "a4221db769004d29b944ba796ad994a5",
      "e530bdcf42454117b631f3b5939c1c62",
      "f84986805a4648b6adc99e420029caaf",
      "9ff958460add463a8805b4c77bed8b8f",
      "e8b374eebc734a2392e802691d422816",
      "b3a8917f0a704bacb55a6d0e2ddb19e9",
      "670bb1297d0343abbe3554425cf12a8d",
      "850ae8dca7c649e3b0088fb351ad2ff0",
      "f2ac1d094e3744bb9cfaf687055fe7c2",
      "fbeee5fbeb994b7c91ba89e6dda5df5b",
      "0bc0c7911cab4bb1bb6f9a980a49cda5",
      "004b4b6727a246589e0636d6fcbb56b5",
      "d666fdaf063546a8bdb653a6297f03a7",
      "621ae0ffc66e4d40bd39ae7312039b2e",
      "0f9f6b07deb94c43829bfdb7f696147b",
      "02560c2f761041c2ba7f50276e29fe0d",
      "0c1bcb9c1ac9470a949e2c0b348e6001",
      "ecadc135b64c4daa833cb73b05ab6347",
      "2dc7f5def7cb49a2a78b6350eb76511d",
      "6c9f7f17fa224906a3feee9c42421a64",
      "868f7d05fa024e82aa8919a21949ce3d",
      "100f718c2ec14d8e8db796e13e585ee1",
      "ba80a9e59da2404d924cbc854b0b55e0",
      "8b096011b9b044cc9a322041b889c48e",
      "d52307c42332479daff8a74cfbcfcca2",
      "80979dc083774223a6e4ed45411d7962",
      "f74169f4a59c4893b4f1f3d685a092e1",
      "e0cb1fff4e554da2a01cd1c44300bfe5",
      "cd86316eef614636911f1528eea0452a",
      "af4b3f9080c14854954420e4e0c064b0",
      "228b78b222324b95809fd5098c1a2d8d",
      "785ebe88c41d463cadee9076d8184fee",
      "f7259f319ec04349a9b71de2428c1113",
      "6bc2345fc49149da8aed004bc530a316",
      "e60706c83051485d86499cf4d4a95ee7",
      "1dc1a1524e2d4673b97d66bd1f1823a7",
      "b7c3d1f94f8149c598160cec1caac3a0",
      "236814b67f2a406cadf3e5e2f64ca22f",
      "be76d01af24d4ab4832ce2807f9ed223",
      "cec602e5fa9d49eb841b10ca78a619ad",
      "e923c066aa284304a7eb7432dd5c9ec5",
      "098c7ea5a36f479a9b25dd1201658588",
      "6a2b8c1f938e4460b5bb4d6c48f58313",
      "425a914d84a540239655c58f146f24e8",
      "86ddb5375fc44693ba196cea0b99eafd",
      "92c9e7e3bd9a439a955b1e1bf50f9c8b",
      "277efa8f99cd4389b30fc5109e9781e3",
      "6af4907c48a74c249f53150a6bc83879",
      "1d93b965bcb34887968e54f1d770cbd6",
      "fbd0a78347d04855b6056bfb9e49fbad",
      "7fe7152dca3e4013a94b604560f68f4d",
      "76c7eacc7d904f2098fa073b5aa0ccc9",
      "1d3b2f3c56684c95a623aaac95f42194",
      "39bf2830971840c9930e76484aed604c",
      "846ac765fb724f6d99dede1f5c1cf0fb",
      "30b79ee2e988401ca8dd03a687eadf5f",
      "b03cb168a2144306a7286f40956c3eeb",
      "5c2671f74b71473fa37b862c9aa0745c"
     ]
    },
    "id": "lSrC098mmqo8",
    "outputId": "ee073222-bd42-4859-ab91-16b23b0acdcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a472d2c5cb344d2a04da2d037f65682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40763 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.1366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved at /content/drive/MyDrive/TinyLLM/model/tinygpt2_epoch7\n",
      "Sample Output:\n",
      "Once upon a time there a girl Lily She to her and mom. loved play her very and her was happy One, went the to park play She a and a. saw big and swing wanted try. mom her said \",, you to the\n",
      "\n",
      "Epoch 8/12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff958460add463a8805b4c77bed8b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40763 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.1300\n",
      "Model checkpoint saved at /content/drive/MyDrive/TinyLLM/model/tinygpt2_epoch8\n",
      "Sample Output:\n",
      "Once upon a time there a girl Lily She to her's to with mom. loved play her and fun. day Lily mom her put her in big and her, closet Lily. closet very and was of. was and.'s said \"ily you\n",
      "\n",
      "Epoch 9/12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9f6b07deb94c43829bfdb7f696147b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40763 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.1248\n",
      "Model checkpoint saved at /content/drive/MyDrive/TinyLLM/model/tinygpt2_epoch9\n",
      "Sample Output:\n",
      "Once upon a time there a girl Lily She to her and brother Tom They to in big. had big and hair Lily a dress Tom. day Lily Tom to to with. wanted play his, Lily to a. said \", you to my.\n",
      "\n",
      "Epoch 10/12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80979dc083774223a6e4ed45411d7962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40763 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.1204\n",
      "Model checkpoint saved at /content/drive/MyDrive/TinyLLM/model/tinygpt2_epoch10\n",
      "Sample Output:\n",
      "Once upon a time there a girl Lily She to her's to with mom. loved play her and fun. day Lily mom her her gave a to. was happy Lily to and with new. played her all long But,'s told to careful not\n",
      "\n",
      "Epoch 11/12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c3d1f94f8149c598160cec1caac3a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40763 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: c8e04fd8-f60e-4a3a-8de7-b7b48d1949f3)')' thrown while requesting GET https://huggingface.co/datasets/roneneldan/TinyStories/resolve/f54c09fd23315a6f9c86f9dc80f725de7d8f9c64/data/train-00003-of-00004-d243063613e5a057.parquet\n",
      "WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: c8e04fd8-f60e-4a3a-8de7-b7b48d1949f3)')' thrown while requesting GET https://huggingface.co/datasets/roneneldan/TinyStories/resolve/f54c09fd23315a6f9c86f9dc80f725de7d8f9c64/data/train-00003-of-00004-d243063613e5a057.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.1167\n",
      "Model checkpoint saved at /content/drive/MyDrive/TinyLLM/model/tinygpt2_epoch11\n",
      "Sample Output:\n",
      "Once upon a time there a girl Lily She to her's to with mom. loved play her and around the. day her said \"ily let go the!\" was happy go her. they to park Lily hermy They on swings slides went and.\n",
      "\n",
      "Epoch 12/12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af4907c48a74c249f53150a6bc83879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40763 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from transformers import GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer from checkpoint (epoch 6)\n",
    "checkpoint_path = Path(\"/content/drive/MyDrive/TinyLLM/model/tinygpt2_epoch6\")\n",
    "model = GPT2LMHeadModel.from_pretrained(checkpoint_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(checkpoint_path)\n",
    "\n",
    "total_samples = 2119719\n",
    "batch_size = 52\n",
    "max_batches_per_epoch = total_samples // batch_size\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training parameters\n",
    "checkpoint_dir = Path(\"/content/drive/MyDrive/TinyLLM/model/\")\n",
    "epochs = 12  # Continue up to epoch 10\n",
    "start_epoch = 6  # Start from epoch 6\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for i, batch in enumerate(tqdm(train_loader, total=max_batches_per_epoch)):\n",
    "        if i >= max_batches_per_epoch:\n",
    "            break\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, labels=labels, attention_mask=attention_mask)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / max_batches_per_epoch\n",
    "    print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save model after each epoch\n",
    "    epoch_checkpoint = checkpoint_dir / f\"tinygpt2_epoch{epoch+1}\"\n",
    "    epoch_checkpoint.mkdir(parents=True, exist_ok=True)\n",
    "    model.save_pretrained(epoch_checkpoint)\n",
    "    tokenizer.save_pretrained(epoch_checkpoint)\n",
    "    print(f\"Model checkpoint saved at {epoch_checkpoint}\")\n",
    "\n",
    "    # Generate sample output\n",
    "    model.eval()\n",
    "    sample_input = tokenizer.encode(\"Once upon a time\", return_tensors=\"pt\").to(device)\n",
    "    generated_ids = model.generate(\n",
    "        sample_input,\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(f\"Sample Output:\\n{generated_text}\")\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7PWMzWJm2Oq"
   },
   "source": [
    "### 7. Generate Text from a Saved GPT-2 Checkpoint\n",
    "\n",
    "1. **Load model and tokenizer**\n",
    "   - Load tokenizer and model from a custom-trained checkpoint (`epoch_5`).\n",
    "\n",
    "2. **Define generation function**\n",
    "   - Encodes input text with attention masks.\n",
    "   - Uses `model.generate` to produce a continuation up to `max_len`.\n",
    "\n",
    "3. **Run examples**\n",
    "   - Generate short story snippets for several starting prompts (e.g., \"Once there was little boy\", \"Once there was a cute little\").\n",
    "\n",
    "- **Related Work:** A Kaggle-hosted version of this project is available here: [TinyStoryLLM by Ashish Jangra](https://www.kaggle.com/models/ashishjangra27/tinystoryllm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "th8m65_pmP55",
    "outputId": "09275596-cb4a-494c-9b03-e4381d2787cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once there was little boy He very. was years and loved play. day he to outside his. saw big and things he never before He a,\n",
      "Once there was little girl Lucy She three old loved play her. day she to outside the and a was in garden She a. was around garden she\n",
      "Once there was a cute girl She to the. was excited go the with mom dad She to park her. they to park. they a slide The\n",
      "Once there was a cute little named. was three old loved explore world One, decided go a. put his on shoes grabbed hat ran and to park\n",
      "Once there was a handsome. was and was happy He a boy He to with big. day he to a. was to a. was to a\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "model_directory = \"epoch_5\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_directory)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_directory)\n",
    "\n",
    "\n",
    "def generate(input_text, max_len):\n",
    "\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "  inputs = tokenizer(\n",
    "      input_text,\n",
    "      return_tensors='pt',\n",
    "      padding=True,\n",
    "      return_attention_mask=True\n",
    "  )\n",
    "\n",
    "  output = model.generate(\n",
    "      input_ids=inputs['input_ids'],\n",
    "      attention_mask=inputs['attention_mask'],\n",
    "      max_length=max_len\n",
    "  )\n",
    "\n",
    "  generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "  return generated_text\n",
    "\n",
    "print(generate(\"Once there was little boy\",30))\n",
    "print(generate(\"Once there was little girl\",30))\n",
    "print(generate(\"Once there was a cute\",30))\n",
    "print(generate(\"Once there was a cute little\",30))\n",
    "print(generate(\"Once there was a handsome\",30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQnz2lvvmHar"
   },
   "source": [
    "### 8. Inference with Pretrained TinyStories Model\n",
    "\n",
    "1. **Load pretrained models**\n",
    "   - `AutoModelForCausalLM`: Loads the `roneneldan/TinyStories-3M` causal language model.  \n",
    "   - `AutoTokenizer`: Uses `EleutherAI/gpt-neo-125M` tokenizer for text processing.\n",
    "\n",
    "2. **Prepare input**\n",
    "   - Encode a simple prompt: `\"Once upon a time there was\"`.\n",
    "\n",
    "3. **Generate text**\n",
    "   - Use `model.generate` with `max_length=1000` to produce a story continuation.\n",
    "\n",
    "4. **Decode output**\n",
    "   - Convert token IDs back to readable text and print the generated story.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UKzLAnBcmHP9",
    "outputId": "2c741366-cd82-453c-9432-409fec22666f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once there was little boy who loved to play with his toys. One day, he was playing with his toy car when he heard a loud noise.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once there was little girl who was three years old. She was very curious and wanted to explore the world.\n",
      "\n",
      "One day, she decided to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once there was a cute little girl named Lily. She loved to play outside in the sunshine. One day, she saw a big, red ball in\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once there was a cute little girl named Lily. She loved to play outside in the sunshine. One day, she saw a big, red ball in\n",
      "Once there was a handsome boy named Tom. He was very brave and always wanted to help others. One day, Tom decided to go on an adventure\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('roneneldan/TinyStories-3M')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "\n",
    "prompt = \"Once upon a time there was\"\n",
    "\n",
    "\n",
    "def generate(input_text, max_len):\n",
    "\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "  inputs = tokenizer(\n",
    "      input_text,\n",
    "      return_tensors='pt',\n",
    "      padding=True,\n",
    "      return_attention_mask=True\n",
    "  )\n",
    "\n",
    "  output = model.generate(\n",
    "      input_ids=inputs['input_ids'],\n",
    "      attention_mask=inputs['attention_mask'],\n",
    "      max_length=max_len\n",
    "  )\n",
    "\n",
    "  generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "  return generated_text\n",
    "\n",
    "  return output_text\n",
    "\n",
    "print(generate(\"Once there was little boy\",30))\n",
    "print(generate(\"Once there was little girl\",30))\n",
    "print(generate(\"Once there was a cute\",30))\n",
    "print(generate(\"Once there was a cute little\",30))\n",
    "print(generate(\"Once there was a handsome\",30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YhTKXeKIF5Z"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89ec3191"
   },
   "source": [
    "### Assignment: Code-Focused Inference\n",
    "\n",
    "Your task is to load a pre-trained GPT-2 model and configure it to answer *only* questions related to Python coding.\n",
    "\n",
    "1. **Load Model and Tokenizer:** Load a suitable pre-trained GPT-2 model and its corresponding tokenizer. You can use `transformers.AutoModelForCausalLM` and `transformers.AutoTokenizer`. A smaller model like `gpt2` or `gpt2-medium` might be sufficient.\n",
    "2. **Implement a Filtering Mechanism:** Before generating a response, check if the input prompt is related to Python coding. You can use simple keyword matching (e.g., \"Python\", \"code\", \"function\", \"class\", \"import\") or a more sophisticated approach using a text classification model (optional).\n",
    "3. **Generate Response:** If the prompt is deemed a Python coding question, generate a response using the loaded GPT-2 model.\n",
    "4. **Handle Non-Coding Questions:** If the prompt is not related to Python coding, return a predefined message indicating that the model can only answer coding questions.\n",
    "5. **Test:** Test your implementation with various prompts, including both Python coding questions and non-coding questions, to ensure the filtering mechanism works correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gs1oWlyQ70X9"
   },
   "source": [
    "# **Assignment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23777697"
   },
   "source": [
    "# Task\n",
    "Create a Python-based solution using the `transformers` library to build a simple chatbot that responds to Python coding questions using a pre-trained language model (e.g., GPT-2). The chatbot should filter out non-coding questions and provide a predefined response for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdfb50ca"
   },
   "source": [
    "## Load model and tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350,
     "referenced_widgets": [
      "fcb6be05aa7e442fb11f54f460f5edff",
      "492ef91cf4744dd39c189802d8e3095a",
      "e54913497c2f4fecadb35e38d80c306f",
      "760f943f82014b458924e43f78cc7b82",
      "30e030e0bdd040c68feaee8b92439b95",
      "5258a71796634febb56ead33416485d4",
      "469983d9207a4776b40ca747e137ed10",
      "23c8375987a44071a095c7b8c04fc18a",
      "013e175a12104bc2911dfca863302639",
      "401bcac76f1f486a8c0edcaedf120050",
      "e2e126f3b1e94a0e9832456c8b627707",
      "75c294e7078840f58d4ae3a8f27a2933",
      "79834d96637348b3a8f98ddf30f3ed28",
      "5bac69b45c4e4df79ccb3675054b1408",
      "5cc0255dbfa042c48d68744c50df739a",
      "3ee8b4d0f8cc4601ab3b282ac73bd98a",
      "f5c69abcab264ba49c33a6e36dfa6386",
      "5fa76de180b745fa923b91614f76ae4e",
      "3eccfbbcd65d402d9a16af8c69d5437a",
      "7e4dc3b68caf4125b97ae0b19e6c602f",
      "542338a3aafe4cf2a5e38a46d236bc80",
      "0a548b41cc954ee3b4dde406a029436e",
      "79a7274d6c9541aa8a4d7ba7d3d2c9d2",
      "73ed3ad18ea34d8eb4b7dc145314a914",
      "3778f43767a44797bf1e18f795107edd",
      "f732f8860a4242258e816b53ab8ba024",
      "0b08074420ee4d319ea3090076e7a460",
      "78f00cad3a5947f08200e48072978f5d",
      "70260bb36d884524b90dc63c68706b62",
      "bccece9ceddc4fd683e5b47d1a21500a",
      "64e7db244b434f16bd4644f786d09d98",
      "cd12dc0a8eb14be181d01125d673e307",
      "5d4390d09d644ceda04f9ea646d6025d",
      "4354d583aa694ed08e8bcf9c8bccccbd",
      "8eb7797cf2f94bc89fd0fc649b8ff7c2",
      "f31a5d3311b3421bb880b1e994378012",
      "c9cb5b855daa48f9978402cb032f140a",
      "33f4c37f106149ed86210f8c53ec20b3",
      "60124be827fb489d939e493ef74f209d",
      "57a873d38de4452e9aaffbc3fd560c7f",
      "3915b18769744672856bc86ba1e5af3f",
      "c6a558ada7e943fb8c980ea86ce405d6",
      "dd9cdcfc335a46a3a55363a56027e30d",
      "f419c1155d2f4a69841c8425a9fbc3b9",
      "a68af3948b644c9187f66421e15cd022",
      "9d783c0bf9834ac7971f81e16c55120d",
      "965e74bc3dff45b1a950a9cdd93d2e62",
      "59404c8875df40e8b8f108734f341ad4",
      "b3d7770ebb304de9b3beaca4096a8677",
      "f25edbff8125431aba17b9899af4743e",
      "c9352a6e223f499ea1fff383f01b9241",
      "15723c74d59744a494162517845db8f6",
      "c4ea09c9811b4686b7fdcdb3c816a29d",
      "b9602ddecd274ac88c211db57cdf1661",
      "4966ff41089f45eaa12a77b8e33251c1",
      "9861f8bffe564d47bb489f7ed96a6b2b",
      "fec1eb03dd544d6f80b9ccf838e53046",
      "fe8f5da603cf4d0b8ab80fa753ef87e1",
      "059868ae928e4d219a16f1f711001bb1",
      "6cf12a7090234859a3776fc771822762",
      "90ed4dd4add24fe8858f3b8d4c1e6ced",
      "d8701746c3a3498b954b5868d04f3336",
      "d4cf804c7c2948f098404a3ee0064538",
      "cae81cfc75c64669b1edcb4620e0af66",
      "7612dee832a34a36be96ef87ec9f2576",
      "84842358c2d24dd085a93eeaa8874bbd",
      "ee4c8d2525da455898de79c4248968f1",
      "af3788de08b24e7b853bcc8049cfa2b0",
      "79d099da2e034b9997abec1ea7af2548",
      "1a37eb268a8d47f98c78082ec6774e16",
      "576c8adc1e734a189e75d0c3f7ed3c73",
      "304deb9fba73491e84cd81c0c6e486ee",
      "a8092234cc434fe18deb7f6d5b411707",
      "d58ac81a85f843569f30e11ff178c388",
      "04ebfba28ffa4f99b7ae0916204ecb27",
      "bcbf852974144ab3aae018e6a2222769",
      "617b05327a7943c7a84c1ab74bf06ed5"
     ]
    },
    "id": "3f4e8bcd",
    "outputId": "7be27d57-0b2f-4b2f-ab89-cfc9595985a5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb6be05aa7e442fb11f54f460f5edff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c294e7078840f58d4ae3a8f27a2933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a7274d6c9541aa8a4d7ba7d3d2c9d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4354d583aa694ed08e8bcf9c8bccccbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68af3948b644c9187f66421e15cd022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9861f8bffe564d47bb489f7ed96a6b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4c8d2525da455898de79c4248968f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3f9809b"
   },
   "source": [
    "## Implement filtering mechanism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1137890e"
   },
   "outputs": [],
   "source": [
    "def is_python_coding_question(prompt: str) -> bool:\n",
    "\n",
    "    python_keywords = [\n",
    "        \"python\", \"code\", \"function\", \"class\", \"import\", \"def\", \"lambda\",\n",
    "        \"list\", \"dict\", \"tuple\", \"set\", \"loop\", \"if\", \"else\", \"elif\",\n",
    "        \"try\", \"except\", \"finally\", \"loop\", \"try\", \"catch\"\n",
    "    ]\n",
    "    prompt_lower = prompt.lower()\n",
    "    for keyword in python_keywords:\n",
    "        if keyword in prompt_lower:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2b5ca3f"
   },
   "source": [
    "## Generate response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "d82bca00"
   },
   "outputs": [],
   "source": [
    "def generate_coding_response(tokenizer, model, prompt: str, max_len: int = 100) -> str:\n",
    "    \"\"\"\n",
    "    Generates a response to a Python coding question using a GPT-2 model,\n",
    "    or returns a predefined message if the input is not a coding question.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: The pre-trained tokenizer.\n",
    "        model: The pre-trained GPT-2 model.\n",
    "        prompt: The input prompt string.\n",
    "        max_len: The maximum length of the generated response.\n",
    "\n",
    "    Returns:\n",
    "        The generated text response or a predefined message.\n",
    "    \"\"\"\n",
    "    if not is_python_coding_question(prompt):\n",
    "        return \"I can only answer Python coding questions. Please ask something related to Python.\"\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=max_len,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ade5cd5"
   },
   "source": [
    "## Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jqYzRWj6iKg4",
    "outputId": "de9aef0a-94b9-48f1-cf56-c7e0f7627440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: How do I define a function in Python?\n",
      "Response: How do I define a function in Python?\n",
      "\n",
      "The Python documentation has a very simple definition of a function. It's called a function, and it's called by the function.\n",
      "\n",
      "The function is called by the function.\n",
      "\n",
      "The function is called by the function.\n",
      "\n",
      "The function is called by the function.\n",
      "\n",
      "The function is called by the function.\n",
      "\n",
      "The function is called by the function.\n",
      "\n",
      "The function is called by the function.\n",
      "\n",
      "The\n",
      "\n",
      "Prompt: What is the capital of France?\n",
      "Response: I can only answer Python coding questions. Please ask something related to Python.\n",
      "\n",
      "Prompt: Explain Python lists.\n",
      "Response: Explain Python lists.\n",
      "\n",
      "The list is a list of Python objects that are used to represent the Python objects in the list.\n",
      "\n",
      "The list is a list of Python objects that are used to represent the Python objects in the list. The list is a list of Python objects that are used to represent the Python objects in the list. The list is a list of Python objects that are used to represent the Python objects in the list. The list is a list of Python objects that are used\n",
      "\n",
      "Prompt: Tell me a story.\n",
      "Response: I can only answer Python coding questions. Please ask something related to Python.\n",
      "\n",
      "Prompt: How to use a for loop in Python?\n",
      "Response: How to use a for loop in Python?\n",
      "\n",
      "The Python library for looping is called for loop. It is a Python library that allows you to use for loops in Python.\n",
      "\n",
      "For example, you can use for loop in Python:\n",
      "\n",
      ">>> for loop in for loop in for loop in for loop in for loop in for loop in for loop in for loop in for loop in for loop in for loop in for loop in for loop in for loop in for loop in for loop\n",
      "\n",
      "Prompt: What is the weather today?\n",
      "Response: I can only answer Python coding questions. Please ask something related to Python.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "test_prompts = [\n",
    "    \"How do I define a function in Python?\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"Explain Python lists.\",\n",
    "    \"Tell me a story.\",\n",
    "    \"How to use a for loop in Python?\",\n",
    "    \"What is the weather today?\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    response = generate_coding_response(tokenizer, model, prompt)\n",
    "    print(f\"Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0da867b6"
   },
   "source": [
    "## Summary:\n",
    "\n",
    "### Data Analysis Key Findings\n",
    "\n",
    "*   The chatbot successfully filters out non-Python coding questions and provides a predefined response (\"I can only answer Python coding questions. Please ask something related to Python.\").\n",
    "*   For prompts identified as Python coding questions, the chatbot attempts to generate a response using the pre-trained GPT-2 model.\n",
    "*   It's not that efficient as of now but still tries to answer python related queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-gm75ou59IlT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
